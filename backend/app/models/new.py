import os
import uuid
import subprocess
import time
import tempfile
import json
import requests
import base64  # CHANGE: Added for Base64 encoding of audio

from pydantic import BaseModel
from typing import List, Optional

# Use the new OpenAI library interface (if needed, ensure it's updated)
from openai import OpenAI  
from moviepy import *

# Load environment variables from .env
from dotenv import load_dotenv
load_dotenv('.env')

# Retrieve API keys from the environment
#from app.config import OPENAI_API_KEY, ELEVENLABS_API_KEY
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

# Initialize ElevenLabs client
from elevenlabs.client import ElevenLabs
from elevenlabs import VoiceSettings

# ---------------------------
# Pydantic models for storyboard
# ---------------------------
class Scene(BaseModel):
    id: str
    narration: str
    manim_prompt: str
    optional_image_prompt: Optional[str] = None

class Storyboard(BaseModel):
    scenes: List[Scene]

# ---------------------------
# Storyboard Generation using OpenAI GPT‑4
# ---------------------------
def generate_storyboard(topic: str) -> Storyboard:
    """
    Generate a storyboard JSON via GPT‑4.
    (For demonstration, we return a fixed sample.)
    """
    sample = {
        "scenes": [
            {
                "id": "scene1",
                "narration": f"Introduction to {topic}. Explaining the basics.",
                "manim_prompt": f"Create an animation that visually explains the basic principles of {topic}.",
                "optional_image_prompt": None
            },
            {
                "id": "scene2",
                "narration": f"Deep dive into {topic}. Discussing detailed aspects.",
                "manim_prompt": f"Generate an animation showing the dynamic behavior of {topic}.",
                "optional_image_prompt": None
            },
            {
                "id": "scene3",
                "narration": f"Conclusion on {topic}. Summarize the key takeaways.",
                "manim_prompt": f"Create a summary animation for {topic}.",
                "optional_image_prompt": None
            }
        ]
    }
    storyboard = Storyboard(**sample)
    return storyboard

# ---------------------------
# ElevenLabs TTS: Convert text to speech and save as file
# ---------------------------
def text_to_speech_file(text: str) -> str:
    elevenlabs_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)
    response = elevenlabs_client.text_to_speech.convert(
        voice_id="pNInz6obpgDQGcFmaJgB",  # Adam pre-made voice
        output_format="mp3_22050_32",
        text=text,
        model_id="eleven_turbo_v2_5",  # turbo model for low latency
        voice_settings=VoiceSettings(
            stability=0.0,
            similarity_boost=1.0,
            style=0.0,
            use_speaker_boost=True,
        ),
    )
    save_file_path = f"{uuid.uuid4()}.wav"
    with open(save_file_path, "wb") as f:
        for chunk in response:
            if chunk:
                f.write(chunk)
    print(f"Audio saved to {save_file_path}")
    return save_file_path

# ---------------------------
# NEW: fal.ai Lipsync Replacement Using Runpod Service
# ---------------------------
# CHANGE: Replacing the entire generate_avatar_video function with new code
def generate_avatar_video(audio_file: str) -> str:
    """
    Generates the avatar video by sending a POST request to a Runpod-hosted API.
    Uses the audio file generated by ElevenLabs (audio_file) and encodes it as a data URI.
    Returns the local path of the generated avatar video.
    """
    # Replace with your public URL from Runpod.
    url = "https://zolpj03o19vuv8-5000.proxy.runpod.net/predict"
    
    # Use a known valid image URL for testing.
    reference_url = "https://raw.githubusercontent.com/adarshxs/temp/refs/heads/main/ladki.jpg"
    
    # Read and encode the local audio file as a Base64 data URI.
    #with open(audio_file, "rb") as af:
    #    audio_data = af.read()
    #audio_base64 = base64.b64encode(audio_data).decode("utf-8")
    #audio_data_uri = f"data:audio/mp3;base64,{audio_base64}"
    
    payload = {
        "reference": reference_url,
        "audio": f"backend/app/models/{audio_file}",
        "animation_mode": "human"
        # Add any additional keys required by your API.
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        print(response.headers)
        print(response.raise_for_status())
        # Save the response content as a video file.
        video_file = f"{uuid.uuid4()}.mp4"
        with open(video_file, "wb") as f:
            f.write(response.content)
        print(f"Success! Video saved as {video_file}")
        return video_file
    except requests.exceptions.RequestException as e:
        print("Error:", e)
        if e.response is not None:
            print("Response details:", e.response.text)
        raise

# ---------------------------
# Generate Manim Code using OpenAI GPT‑4
# ---------------------------
def generate_manim_code(scene_text: str) -> str:
    client = OpenAI(api_key=OPENAI_API_KEY)
    prompt = f"Generate manim code that produces an animation visualizing: {scene_text}"
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # Using gpt-3.5-turbo; update as needed
        messages=[{"role": "user", "content": prompt}]
    )
    manim_code = response.choices[0].message.content.strip()
    print("Manim code generated.")
    return manim_code

# ---------------------------
# Render Manim Video: Save manim code to a file and call manim CLI to render it.
# ---------------------------
def render_manim_video(manim_code: str, output_filename: str) -> str:
    temp_dir = tempfile.gettempdir()
    code_filename = os.path.join(temp_dir, f"{uuid.uuid4()}_animation.py")
    with open(code_filename, "w") as f:
        f.write(manim_code)
    # Assume the generated code defines a Scene class called "Scene"
    cmd = ["manim", "-ql", code_filename, "Scene", "-o", output_filename]
    print("Rendering manim animation...")
    subprocess.run(cmd, check=True)
    print(f"Manim animation saved to {output_filename}")
    return output_filename

# ---------------------------
# Compose a Scene: Overlay manim video (upper half) and avatar video (lower half)
# ---------------------------
def compose_scene(manim_video: str, avatar_video: str) -> VideoFileClip:
    final_width, final_height = 1280, 720

    clip_manim = VideoFileClip(manim_video).resize((final_width, final_height // 2))
    clip_avatar = VideoFileClip(avatar_video).resize((final_width, final_height // 2))
    
    duration = min(clip_manim.duration, clip_avatar.duration)
    background = ColorClip(size=(final_width, final_height), color=(0, 0, 0), duration=duration)
    
    clip_manim = clip_manim.set_position(("center", 0))
    clip_avatar = clip_avatar.set_position(("center", final_height // 2))
    
    composite = CompositeVideoClip([background, clip_manim.set_start(0), clip_avatar.set_start(0)])
    composite = composite.set_duration(duration)
    return composite

# ---------------------------
# Create Final Reel: Process all scenes and stitch them together.
# ---------------------------
def create_final_reel(storyboard: Storyboard) -> str:
    scene_clips = []
    for scene in storyboard.scenes:
        print(f"Processing {scene.id}...")
        # Generate narration audio for the scene.
        audio_file = text_to_speech_file(scene.narration)
        # Generate avatar video using the new lipsync function.
        avatar_video = generate_avatar_video(audio_file)
        # Generate manim code for the scene’s visual.
        manim_code = generate_manim_code(scene.manim_prompt)
        # Render the manim animation video.
        manim_video_file = f"{uuid.uuid4()}_manim.mp4"
        render_manim_video(manim_code, manim_video_file)
        # Compose the scene clip (manim on top, avatar below).
        scene_clip = compose_scene(manim_video_file, avatar_video)
        scene_clips.append(scene_clip)
    
    final_reel = concatenate_videoclips(scene_clips, method="compose")
    output_reel = "final_reel.mp4"
    final_reel.write_videofile(output_reel, codec="libx264", audio_codec="aac")
    print("Final reel rendering complete.")
    return output_reel

# ---------------------------
# Main orchestration function
# ---------------------------
def main(topic_prompt: str):
    print("Generating storyboard...")
    storyboard = generate_storyboard(topic_prompt)
    print("Storyboard:")
    print(storyboard.model_dump_json(indent=2))  # Using model_dump_json for pretty printing with Pydantic v2
    
    print("Creating final reel...")
    reel_path = create_final_reel(storyboard)
    print(f"Final reel available at: {reel_path}")

if __name__ == "__main__":
    main("Quantum Entanglement")
